Model,Publication date,Training power draw (watts)
Llama 3.1-405B,7/23/24,25280251.59
Nemotron-4 340B,6/14/24,9483521.861
MegaScale (Production),2/23/24,10849658.52
Gemini 1.0 Ultra,12/6/23,24175462.28
Inflection-2,11/22/23,7732579.928
Amazon Titan,9/28/23,12166402.81
Falcon-180B,9/6/23,3622388.562
GPT-4,3/15/23,22146708.68
U-PaLM (540B),10/20/22,218023.4995
Flan-PaLM 540B,10/20/22,218023.4995
BlenderBot 3,8/10/22,113634.052
Minerva (540B),6/29/22,436538.0064
OPT-175B,5/2/22,909984.4297
PaLM (540B),4/4/22,2621496.055
LaMDA,2/10/22,1024572.282
ERNIE 3.0 Titan,12/23/21,2106.480894
GLaM,12/13/21,437413.9514
Gopher (280B),12/8/21,4100965.607
Megatron-Turing NLG 530B,10/11/21,3989424.742
ProtT5-XXL,5/4/21,513779.0241
Meta Pseudo Labels,3/1/21,1028249.308
Switch,1/11/21,1028782.017
DALL-E,1/5/21,571581.9188
GShard (dense),6/30/20,1030932.089
GPT-3 175B (davinci),5/28/20,5595164.713
Meena,1/28/20,1032664.632
OpenAI Five Rerun,12/13/19,286996.7074
OpenAI Five,12/13/19,860990.1223
Noisy Student (L2),11/11/19,1033553.974
AlphaStar,10/30/19,387634.3155
T5-11B,10/23/19,516885.9122
Megatron-LM (8.3B),9/17/19,287273.8627
Megatron-BERT,9/17/19,287273.8627
RoBERTa Large,7/1/19,575049.4486
BigGAN-deep 512x512,9/28/18,259587.6723
ResNeXt-101 32x48d,5/2/18,227533.9495
AlphaZero,12/5/17,5647.409436
JFT,7/10/17,33978.9516
MoE-Multi,1/23/17,35592.29371
PolyNet,11/17/16,18174.38215
Xception,10/7/16,40913.2093
DeepSpeech2 (English),12/8/15,9126.870568
VGG16,9/4/14,2295.573892
Deep Autoencoders,4/29/11,476.592173